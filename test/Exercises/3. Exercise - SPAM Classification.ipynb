{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-o2cEpeQZjs"
      },
      "source": [
        "# Machine Learning - Exercise 3\n",
        "# SMS SPAM classification\n",
        "\n",
        "To perform the experiments on the SMSSpamCollection dataset you need to set-up your Colab such that it is able to load the desired data. To achieve this, you need to perform the following actions:\n",
        "\n",
        "*   download the dataset available at this [link](https://drive.google.com/a/diag.uniroma1.it/file/d/17YZemn1MidhFA0-wenfVolZAwclLRUXM/view)\n",
        "*   copy the dataset in a folder of your personal Drive\n",
        "*   mount your Google Drive (more details will follow)\n",
        "*   set the correct path for loading the dataset (more details will follow)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z62YJE78EcK9"
      },
      "source": [
        "## Import needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJXZGno8QYOn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9821411e-e2e4-4be4-e1b2-fc0894b6a346"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import *\n",
        "from sklearn.naive_bayes import *\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "print('Libraries imported.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnm5HakC6dO0"
      },
      "source": [
        "## Load data\n",
        "\n",
        "Mount Google Drive by following the instructions given at the provided link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqhr4vhcRD39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7c93717-4357-4669-cd9f-8edd9cbc3083"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otk6cVVo6mR8"
      },
      "source": [
        "To load the file set the correct path of the dataset located in your drive. Once mounted, your drive works like a Linux system, so you can check folders etc... running commands like `ls` or `cd` preceded by `%`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvPjvjXGQ0sw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf4e5641-d022-4282-efbe-2703293d9d94"
      },
      "source": [
        "# example path of dataset copied in My Drive folder: /content/drive/My Drive/SMSSpamCollection'\n",
        "filename = '/content/drive/Shared drives/Machine Learning/Datasets/SMSSpamCollection'\n",
        "db = pd.read_csv(filename, sep='\\t', header=None, names=['label', 'text'])\n",
        "print('File loaded: %d samples.' %(len(db.label)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File loaded: 5572 samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns_oBwUo64tF"
      },
      "source": [
        "Show a random sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhl2FLCKSUmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb7d2c28-e4f7-4ec3-a2a2-287bfc779bd7"
      },
      "source": [
        "id = random.randrange(0,len(db.label))\n",
        "print('%d %s %s' %(id,db.label[id],db.text[id]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2689 ham Yes I know the cheesy songs from frosty the snowman :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uBD7RsJ7bzB"
      },
      "source": [
        "## Choose vectorizer\n",
        "\n",
        "Compute vectorizer terms for all messages. More info:\n",
        "\n",
        "\n",
        "\n",
        "*   [Hashing](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)\n",
        "*   [Count](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
        "*   [Tfid](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKWEucDKWnDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13d30405-0c46-43ef-e752-ce310f6556e6"
      },
      "source": [
        "vectorizer_type = \"count\" # \"hashing\", \"count\" or \"tfid\"\n",
        "\n",
        "if vectorizer_type == \"hashing\":\n",
        "  vectorizer = HashingVectorizer(stop_words='english') # multivariate\n",
        "elif vectorizer_type == \"count\":\n",
        "  vectorizer = CountVectorizer(stop_words='english') # multinomial\n",
        "elif vectorizer_type == \"tfid\":\n",
        "  vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "X_all = vectorizer.fit_transform(db.text)\n",
        "y_all = db.label\n",
        "\n",
        "print(X_all.shape)\n",
        "print(y_all.shape)\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5572, 8444)\n",
            "(5572,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7oczFvA7nXb"
      },
      "source": [
        "## Split data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Yw3Fz1NSu01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5337cd5-7afa-4671-d8b5-958d639ff8af"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, \n",
        "          test_size=0.2, random_state=16)\n",
        "\n",
        "print(\"Train: %d - Test: %d\" %(X_train.shape[0],X_test.shape[0]))\n",
        "\n",
        "#id = random.randrange(0,X_train.shape[0])\n",
        "#print('%d ' %(id))\n",
        "#print('%d %s %s' %(id,str(y_train[id]),str(X_train[id])))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2666 \n",
            "Train: 4457 - Test: 1115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y66oz8ep57xg"
      },
      "source": [
        "## Create and fit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk4BBWhpUrQO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cfc13d0-8bc4-475e-d062-06abb82220bc"
      },
      "source": [
        "model_type = \"multinomial\" # \"bernoulli\" or \"multinomial\"\n",
        "\n",
        "if model_type == \"bernoulli\":\n",
        "  model = BernoulliNB().fit(X_train, y_train)\n",
        "  print('Bernoulli Model created')\n",
        "elif model_type == \"multinomial\":\n",
        "  model = MultinomialNB().fit(X_train, y_train)\n",
        "  print('Multinomial Model created')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multinomial Model created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VBrexa46DmE"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2dR-PQtaiJC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c49e2f03-dca3-4bdc-b1a3-1b309c7c1f7d"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[962   9]\n",
            " [ 10 134]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.99      0.99       971\n",
            "        spam       0.94      0.93      0.93       144\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.96      0.96      0.96      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaiH-mlO6I-9"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD7Le-lVX9M8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a13f765-f74a-4097-bbf9-a37f424ef4f1"
      },
      "source": [
        "smsnew1 = np.array(['Hello, did you solve ML exercise?'])\n",
        "xnew1 = vectorizer.transform(smsnew1)\n",
        "ynew1 = model.predict(xnew1)\n",
        "print('%s %s' %(smsnew1,ynew1))\n",
        "\n",
        "smsnew2 = np.array(['You won $1,000! Call now 1-800-1234567'])\n",
        "xnew2 = vectorizer.transform(smsnew2)\n",
        "ynew2 = model.predict(xnew2)\n",
        "print('%s %s' %(smsnew2,ynew2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello, did you solve ML exercise?'] ['ham']\n",
            "['You won $1,000! Call now 1-800-1234567'] ['spam']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjYkT9M_N5Wb"
      },
      "source": [
        "## Home Exercises\n",
        "\n",
        "**Question 1**\n",
        "\n",
        "Design and implement an evaluation procedure to assess and compare the performance of the three vectorizers and the two models proposed above.\n",
        "\n",
        "\n"
      ]
    }
  ]
}